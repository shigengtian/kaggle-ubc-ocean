{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cb761a2-0e8e-49c7-a954-9f32f546fe8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import pyvips\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "import torchvision\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc623f1f-a183-4a98-a2ac-084a97808590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name = \"tf_efficientnetv2_s_in21ft1k\"\n",
    "    selected_folds = [0, 1, 2, 3, 4]\n",
    "    img_size = 512\n",
    "    # batch_size and epochs\n",
    "    batch_size = 8\n",
    "    num_workers = 4\n",
    "    label_dict = {0: \"CC\", 1: \"EC\", 2: \"HGSC\", 3: \"LGSC\", 4: \"MC\", 5:\"Other\"}\n",
    "    num_classes = 5\n",
    "    \n",
    "    seg_model_name = \"efficientnet-b0\"\n",
    "    seg_model_weight = \"exp-seg/seg-fold-0.pth\"\n",
    "    \n",
    "    clc_model_name = \"tf_efficientnetv2_s_in21ft1k\"\n",
    "    clc_model_weight = \"/kaggle/input/ubc-ocean/exp06-fold-0-acc0.77.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a3de084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a009db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Debug= True\n",
    "data_dir = Path(\"dataset\")\n",
    "test_df = pd.read_csv(data_dir / \"test.csv\")\n",
    "test_df[\"is_tma\"] =test_df[\"image_width\"] < 6000\n",
    "img_path = \"test_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14315e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Debug:\n",
    "    train_df = pd.read_csv(data_dir / \"train.csv\" )\n",
    "    test_df = train_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c7e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = Path(\"test_tiles\")\n",
    "target_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de494331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(row, data_dir, target_path):\n",
    "    tma = row[\"is_tma\"]\n",
    "    image_id = row[\"image_id\"]\n",
    "    \n",
    "    img_path = data_dir / \"train_images\" / f\"{row.image_id}.png\"\n",
    "    \n",
    "\n",
    "\n",
    "    new_size = (512, 512)\n",
    "    tile_size = 2048\n",
    "    if ~tma:\n",
    "        img = pyvips.Image.new_from_file(img_path)\n",
    "        height = img.height\n",
    "        width = img.width\n",
    "        # Iterate over the image in tiles\n",
    "        for y in range(0, height, tile_size):\n",
    "            for x in range(0, width, tile_size):\n",
    "                # Extract a tile from the image and mask\n",
    "                img_tile = img.crop(\n",
    "                    x, y, min(tile_size, width - x), min(tile_size, height - y)\n",
    "                ).numpy()\n",
    "\n",
    "                gray = cv2.cvtColor(img_tile, cv2.COLOR_BGR2GRAY)\n",
    "                _, binary_image = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                # black area ratio threshold start from here\n",
    "                black_pixels = np.count_nonzero(binary_image == 0)\n",
    "                total_pixels = np.prod(binary_image.shape[:2])\n",
    "                black_area_ratio = black_pixels / total_pixels\n",
    "\n",
    "\n",
    "                if black_area_ratio > 0.3:\n",
    "#                     print(f\"black_area_ratio: {black_area_ratio}\")\n",
    "                    continue\n",
    "\n",
    "                tile_save_path = target_path / f\"{image_id}_tile_{x}_{y}.png\"\n",
    "                Image.fromarray(img_tile).resize(new_size, Image.LANCZOS).save(\n",
    "                            tile_save_path\n",
    "                        )\n",
    "        \n",
    "\n",
    "    else:\n",
    "        img = Image.open(img_path)\n",
    "        target_path = target_path / f\"{image_id}_tile.png\"\n",
    "        Image.fromarray(img_tile).resize(new_size, Image.LANCZOS).save(\n",
    "                    target_path\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8344da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [09:17<00:00, 27.88s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_processes = 4\n",
    "_ = Parallel(n_jobs=num_processes, backend=\"multiprocessing\")(\n",
    "    delayed(process_image)(row, data_dir, target_path)\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1539b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_files = glob(str(target_path / \"*.png\"))\n",
    "tile_df = pd.DataFrame(tile_files, columns=[\"tile_path\"])\n",
    "tile_df[\"image_id\"] =   tile_df[\"tile_path\"].apply(lambda x: x.split(\"/\")[-1].split(\"_\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82e653b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(CFG.img_size, CFG.img_size),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0,\n",
    "                p=1.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        p=1.0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "285e7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UBCSegDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.img_paths = df[\"tile_path\"].values\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_path = self.img_paths[index]\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return {\n",
    "            \"image\": img,\n",
    "        }\n",
    "    \n",
    "def seg_infer(test_loader, model):\n",
    "    \n",
    "    mask_ratio = []\n",
    "    for index, data in enumerate(test_loader):\n",
    "        images = data[\"image\"].to(device)\n",
    "        batch_size = images.size(0) \n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        probs = probs.detach().cpu().numpy()\n",
    "\n",
    "        threshold = 0.33\n",
    "        binary_masks = (probs > threshold).astype(int)\n",
    "        \n",
    "         \n",
    "        for _index in range(batch_size):\n",
    "            true_pixel_ratio = np.count_nonzero(binary_masks[_index]) /(512*512)\n",
    "            mask_ratio.append(true_pixel_ratio)\n",
    "    #     for index, image_id in enumerate(image_ids):\n",
    "    #         _image_ids.append(str(image_id))\n",
    "    #         target_path = f\"{image_id}_seg.npy\"\n",
    "    #         _target_paths.append(target_path)\n",
    "    #         np.save(target_path, binary_masks[index][0])\n",
    "    \n",
    "    # df[\"seg_paths\"] = _target_paths\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return mask_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b88fd2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg_model_weight loaded\n"
     ]
    }
   ],
   "source": [
    "seg_model = smp.Unet(\n",
    "    encoder_name=CFG.seg_model_name,\n",
    "    encoder_weights=None,\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None,\n",
    ")\n",
    "\n",
    "seg_model.to(device)\n",
    "seg_model.eval()\n",
    "seg_model.load_state_dict(torch.load(CFG.seg_model_weight))\n",
    "print(\"seg_model_weight loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92433612",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = UBCSegDataset(tile_df, transforms=get_transforms())\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, \n",
    "                            num_workers=4, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dda4a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tile_path</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_tiles/1020_tile_26624_24576.png</td>\n",
       "      <td>1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_tiles/706_tile_65536_4096.png</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_tiles/706_tile_30720_6144.png</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_tiles/1660_tile_67584_16384.png</td>\n",
       "      <td>1660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_tiles/281_tile_8192_6144.png</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2877</th>\n",
       "      <td>test_tiles/431_tile_34816_22528.png</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>test_tiles/286_tile_22528_18432.png</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>test_tiles/1666_tile_0_8192.png</td>\n",
       "      <td>1666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>test_tiles/66_tile_40960_14336.png</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>test_tiles/431_tile_14336_20480.png</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2882 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 tile_path image_id\n",
       "0     test_tiles/1020_tile_26624_24576.png     1020\n",
       "1       test_tiles/706_tile_65536_4096.png      706\n",
       "2       test_tiles/706_tile_30720_6144.png      706\n",
       "3     test_tiles/1660_tile_67584_16384.png     1660\n",
       "4        test_tiles/281_tile_8192_6144.png      281\n",
       "...                                    ...      ...\n",
       "2877   test_tiles/431_tile_34816_22528.png      431\n",
       "2878   test_tiles/286_tile_22528_18432.png      286\n",
       "2879       test_tiles/1666_tile_0_8192.png     1666\n",
       "2880    test_tiles/66_tile_40960_14336.png       66\n",
       "2881   test_tiles/431_tile_14336_20480.png      431\n",
       "\n",
       "[2882 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65f46066",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ratio = seg_infer(test_loader=test_loader, model=seg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6cc1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_df[\"mask_ratio\"] = mask_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8155c858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_id\n",
       "1020    1.000000\n",
       "1080    1.000000\n",
       "1101    1.000000\n",
       "1252    1.000000\n",
       "1289    0.991673\n",
       "1295    1.000000\n",
       "1660    1.000000\n",
       "1666    1.000000\n",
       "1774    1.000000\n",
       "1925    1.000000\n",
       "1943    0.995358\n",
       "1952    1.000000\n",
       "281     0.893536\n",
       "286     1.000000\n",
       "4       1.000000\n",
       "431     1.000000\n",
       "66      1.000000\n",
       "706     1.000000\n",
       "91      0.268433\n",
       "970     1.000000\n",
       "Name: mask_ratio, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tile_df.groupby(\"image_id\")[\"mask_ratio\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014db7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aafd308b-d8a3-49bf-9144-83040458d642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class UBCSegDataset(Dataset):\n",
    "#     def __init__(self, img_paths, transforms=None):\n",
    "#         self.img_paths = img_paths\n",
    "#         self.transforms = transforms\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "        \n",
    "#         img_path = self.img_paths[index]\n",
    "\n",
    "#         img = cv2.imread(str(img_path))\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "#         # image_id = self.image_ids[index]\n",
    "        \n",
    "#         if self.transforms:\n",
    "#             img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "#         return {\n",
    "#             \"image\": img,\n",
    "#         }\n",
    "    \n",
    "\n",
    "# def seg_infer(df):\n",
    "    \n",
    "#     _image_ids = []\n",
    "#     _target_paths = []\n",
    "    \n",
    "#     data_transforms = {\n",
    "#         \"valid\": A.Compose([\n",
    "#             A.Resize(CFG.img_size, CFG.img_size),\n",
    "#             A.Normalize(\n",
    "#                     mean=[0.485, 0.456, 0.406], \n",
    "#                     std=[0.229, 0.224, 0.225], \n",
    "#                     max_pixel_value=255.0, \n",
    "#                     p=1.0\n",
    "#                 ),\n",
    "#             ToTensorV2()], p=1.)\n",
    "#     }\n",
    "\n",
    "#     valid_dataset = UBCSegDataset(test_df, transforms=data_transforms[\"valid\"])\n",
    "\n",
    "#     valid_loader = DataLoader(\n",
    "#         valid_dataset,\n",
    "#         batch_size=CFG.batch_size,\n",
    "#         shuffle=False,\n",
    "#         num_workers=CFG.num_workers,\n",
    "#         pin_memory=True,\n",
    "#         drop_last=False,\n",
    "#     )\n",
    "    \n",
    "#     model_name = \"efficientnet-b0\"\n",
    "#     model_weight = \"exp-seg/seg-fold-0.pth\"\n",
    "    \n",
    "#     model = smp.Unet(\n",
    "#         encoder_name=model_name,\n",
    "#         encoder_weights=None,\n",
    "#         in_channels=3,\n",
    "#         classes=1,\n",
    "#         activation=None,\n",
    "#     )\n",
    "\n",
    "    \n",
    "#     model.to(device)\n",
    "#     model.load_state_dict(torch.load(model_weight))\n",
    "#     print(\"weights loaded\")\n",
    "\n",
    "#     for index, data in enumerate(valid_loader):\n",
    "#         images = data[\"image\"].to(device)\n",
    "#         image_ids = data[\"image_id\"]\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(images)\n",
    "\n",
    "#         probs = torch.sigmoid(outputs)\n",
    "#         probs = probs.detach().cpu().numpy()\n",
    "\n",
    "#         threshold = 0.50\n",
    "#         binary_masks = (probs > threshold).astype(int)\n",
    "#         for index, image_id in enumerate(image_ids):\n",
    "#             _image_ids.append(str(image_id))\n",
    "#             target_path = f\"infer_seg/{image_id}_seg.npy\"\n",
    "#             _target_paths.append(target_path)\n",
    "#             np.save(target_path, binary_masks[index][0])\n",
    "    \n",
    "# #     rst = pd.DataFrame()\n",
    "# #     rst[\"image_id\"] = _image_ids\n",
    "#     df[\"seg_paths\"] = _target_paths\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b6abafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_image(row):\n",
    "#     image_id = row.image_id\n",
    "#     image_path = row.file_path\n",
    "#     is_tma = row.is_tma\n",
    "\n",
    "#     img = cv2.imread(str(image_path))\n",
    "\n",
    "#     height, width = img.shape[:2]\n",
    "    \n",
    "#     img_target_path = Path(\"tiles\")\n",
    "#     img_target_path.mkdir(parents=True, exist_ok=True)\n",
    "#     tile_size = 2048\n",
    "#     rows = height // tile_size\n",
    "#     cols = width // tile_size\n",
    "\n",
    "#     if is_tma:\n",
    "#         print(\"is tma\")\n",
    "#     else:\n",
    "#         for i in range(rows):\n",
    "#             for j in range(cols):                     \n",
    "#                 tile = img[\n",
    "#                     i * tile_size: (i + 1) * tile_size,\n",
    "#                     j * tile_size: (j + 1) * tile_size,\n",
    "#                 ]\n",
    " \n",
    "#                 tile_filename = f\"{image_id}_{i}_{j}.png\"\n",
    "#                 tile_path = img_target_path / tile_filename\n",
    "#                 cv2.imwrite(str(tile_path), tile)\n",
    "\n",
    "#                 del tile\n",
    "#                 gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92b8cfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/train_images/6175.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n"
     ]
    }
   ],
   "source": [
    "# for index, row in test_df.iterrows():\n",
    "\n",
    "#     print(row.file_path)\n",
    "#     process_image(row)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaa64f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tiles/6175_0_0.png',\n",
       " 'tiles/6175_0_1.png',\n",
       " 'tiles/6175_0_10.png',\n",
       " 'tiles/6175_0_11.png',\n",
       " 'tiles/6175_0_12.png',\n",
       " 'tiles/6175_0_13.png',\n",
       " 'tiles/6175_0_14.png',\n",
       " 'tiles/6175_0_15.png',\n",
       " 'tiles/6175_0_16.png',\n",
       " 'tiles/6175_0_17.png',\n",
       " 'tiles/6175_0_18.png',\n",
       " 'tiles/6175_0_19.png',\n",
       " 'tiles/6175_0_2.png',\n",
       " 'tiles/6175_0_20.png',\n",
       " 'tiles/6175_0_21.png',\n",
       " 'tiles/6175_0_22.png',\n",
       " 'tiles/6175_0_23.png',\n",
       " 'tiles/6175_0_24.png',\n",
       " 'tiles/6175_0_25.png',\n",
       " 'tiles/6175_0_26.png',\n",
       " 'tiles/6175_0_27.png',\n",
       " 'tiles/6175_0_28.png',\n",
       " 'tiles/6175_0_3.png',\n",
       " 'tiles/6175_0_4.png',\n",
       " 'tiles/6175_0_5.png',\n",
       " 'tiles/6175_0_6.png',\n",
       " 'tiles/6175_0_7.png',\n",
       " 'tiles/6175_0_8.png',\n",
       " 'tiles/6175_0_9.png',\n",
       " 'tiles/6175_10_0.png',\n",
       " 'tiles/6175_10_1.png',\n",
       " 'tiles/6175_10_10.png',\n",
       " 'tiles/6175_10_11.png',\n",
       " 'tiles/6175_10_12.png',\n",
       " 'tiles/6175_10_13.png',\n",
       " 'tiles/6175_10_14.png',\n",
       " 'tiles/6175_10_15.png',\n",
       " 'tiles/6175_10_16.png',\n",
       " 'tiles/6175_10_17.png',\n",
       " 'tiles/6175_10_18.png',\n",
       " 'tiles/6175_10_19.png',\n",
       " 'tiles/6175_10_2.png',\n",
       " 'tiles/6175_10_20.png',\n",
       " 'tiles/6175_10_21.png',\n",
       " 'tiles/6175_10_22.png',\n",
       " 'tiles/6175_10_23.png',\n",
       " 'tiles/6175_10_24.png',\n",
       " 'tiles/6175_10_25.png',\n",
       " 'tiles/6175_10_26.png',\n",
       " 'tiles/6175_10_27.png',\n",
       " 'tiles/6175_10_28.png',\n",
       " 'tiles/6175_10_3.png',\n",
       " 'tiles/6175_10_4.png',\n",
       " 'tiles/6175_10_5.png',\n",
       " 'tiles/6175_10_6.png',\n",
       " 'tiles/6175_10_7.png',\n",
       " 'tiles/6175_10_8.png',\n",
       " 'tiles/6175_10_9.png',\n",
       " 'tiles/6175_11_0.png',\n",
       " 'tiles/6175_11_1.png',\n",
       " 'tiles/6175_11_10.png',\n",
       " 'tiles/6175_11_11.png',\n",
       " 'tiles/6175_11_12.png',\n",
       " 'tiles/6175_11_13.png',\n",
       " 'tiles/6175_11_14.png',\n",
       " 'tiles/6175_11_15.png',\n",
       " 'tiles/6175_11_16.png',\n",
       " 'tiles/6175_11_17.png',\n",
       " 'tiles/6175_11_18.png',\n",
       " 'tiles/6175_11_19.png',\n",
       " 'tiles/6175_11_2.png',\n",
       " 'tiles/6175_11_20.png',\n",
       " 'tiles/6175_11_21.png',\n",
       " 'tiles/6175_11_22.png',\n",
       " 'tiles/6175_11_23.png',\n",
       " 'tiles/6175_11_24.png',\n",
       " 'tiles/6175_11_25.png',\n",
       " 'tiles/6175_11_26.png',\n",
       " 'tiles/6175_11_27.png',\n",
       " 'tiles/6175_11_28.png',\n",
       " 'tiles/6175_11_3.png',\n",
       " 'tiles/6175_11_4.png',\n",
       " 'tiles/6175_11_5.png',\n",
       " 'tiles/6175_11_6.png',\n",
       " 'tiles/6175_11_7.png',\n",
       " 'tiles/6175_11_8.png',\n",
       " 'tiles/6175_11_9.png',\n",
       " 'tiles/6175_12_0.png',\n",
       " 'tiles/6175_12_1.png',\n",
       " 'tiles/6175_12_10.png',\n",
       " 'tiles/6175_12_11.png',\n",
       " 'tiles/6175_12_12.png',\n",
       " 'tiles/6175_12_13.png',\n",
       " 'tiles/6175_12_14.png',\n",
       " 'tiles/6175_12_15.png',\n",
       " 'tiles/6175_12_16.png',\n",
       " 'tiles/6175_12_17.png',\n",
       " 'tiles/6175_12_18.png',\n",
       " 'tiles/6175_12_19.png',\n",
       " 'tiles/6175_12_2.png',\n",
       " 'tiles/6175_12_20.png',\n",
       " 'tiles/6175_12_21.png',\n",
       " 'tiles/6175_12_22.png',\n",
       " 'tiles/6175_12_23.png',\n",
       " 'tiles/6175_12_24.png',\n",
       " 'tiles/6175_12_25.png',\n",
       " 'tiles/6175_12_26.png',\n",
       " 'tiles/6175_12_27.png',\n",
       " 'tiles/6175_12_28.png',\n",
       " 'tiles/6175_12_3.png',\n",
       " 'tiles/6175_12_4.png',\n",
       " 'tiles/6175_12_5.png',\n",
       " 'tiles/6175_12_6.png',\n",
       " 'tiles/6175_12_7.png',\n",
       " 'tiles/6175_12_8.png',\n",
       " 'tiles/6175_12_9.png',\n",
       " 'tiles/6175_13_0.png',\n",
       " 'tiles/6175_13_1.png',\n",
       " 'tiles/6175_13_10.png',\n",
       " 'tiles/6175_13_11.png',\n",
       " 'tiles/6175_13_12.png',\n",
       " 'tiles/6175_13_13.png',\n",
       " 'tiles/6175_13_14.png',\n",
       " 'tiles/6175_13_15.png',\n",
       " 'tiles/6175_13_16.png',\n",
       " 'tiles/6175_13_17.png',\n",
       " 'tiles/6175_13_18.png',\n",
       " 'tiles/6175_13_19.png',\n",
       " 'tiles/6175_13_2.png',\n",
       " 'tiles/6175_13_20.png',\n",
       " 'tiles/6175_13_21.png',\n",
       " 'tiles/6175_13_22.png',\n",
       " 'tiles/6175_13_23.png',\n",
       " 'tiles/6175_13_24.png',\n",
       " 'tiles/6175_13_25.png',\n",
       " 'tiles/6175_13_26.png',\n",
       " 'tiles/6175_13_27.png',\n",
       " 'tiles/6175_13_28.png',\n",
       " 'tiles/6175_13_3.png',\n",
       " 'tiles/6175_13_4.png',\n",
       " 'tiles/6175_13_5.png',\n",
       " 'tiles/6175_13_6.png',\n",
       " 'tiles/6175_13_7.png',\n",
       " 'tiles/6175_13_8.png',\n",
       " 'tiles/6175_13_9.png',\n",
       " 'tiles/6175_1_0.png',\n",
       " 'tiles/6175_1_1.png',\n",
       " 'tiles/6175_1_10.png',\n",
       " 'tiles/6175_1_11.png',\n",
       " 'tiles/6175_1_12.png',\n",
       " 'tiles/6175_1_13.png',\n",
       " 'tiles/6175_1_14.png',\n",
       " 'tiles/6175_1_15.png',\n",
       " 'tiles/6175_1_16.png',\n",
       " 'tiles/6175_1_17.png',\n",
       " 'tiles/6175_1_18.png',\n",
       " 'tiles/6175_1_19.png',\n",
       " 'tiles/6175_1_2.png',\n",
       " 'tiles/6175_1_20.png',\n",
       " 'tiles/6175_1_21.png',\n",
       " 'tiles/6175_1_22.png',\n",
       " 'tiles/6175_1_23.png',\n",
       " 'tiles/6175_1_24.png',\n",
       " 'tiles/6175_1_25.png',\n",
       " 'tiles/6175_1_26.png',\n",
       " 'tiles/6175_1_27.png',\n",
       " 'tiles/6175_1_28.png',\n",
       " 'tiles/6175_1_3.png',\n",
       " 'tiles/6175_1_4.png',\n",
       " 'tiles/6175_1_5.png',\n",
       " 'tiles/6175_1_6.png',\n",
       " 'tiles/6175_1_7.png',\n",
       " 'tiles/6175_1_8.png',\n",
       " 'tiles/6175_1_9.png',\n",
       " 'tiles/6175_2_0.png',\n",
       " 'tiles/6175_2_1.png',\n",
       " 'tiles/6175_2_10.png',\n",
       " 'tiles/6175_2_11.png',\n",
       " 'tiles/6175_2_12.png',\n",
       " 'tiles/6175_2_13.png',\n",
       " 'tiles/6175_2_14.png',\n",
       " 'tiles/6175_2_15.png',\n",
       " 'tiles/6175_2_16.png',\n",
       " 'tiles/6175_2_17.png',\n",
       " 'tiles/6175_2_18.png',\n",
       " 'tiles/6175_2_19.png',\n",
       " 'tiles/6175_2_2.png',\n",
       " 'tiles/6175_2_20.png',\n",
       " 'tiles/6175_2_21.png',\n",
       " 'tiles/6175_2_22.png',\n",
       " 'tiles/6175_2_23.png',\n",
       " 'tiles/6175_2_24.png',\n",
       " 'tiles/6175_2_25.png',\n",
       " 'tiles/6175_2_26.png',\n",
       " 'tiles/6175_2_27.png',\n",
       " 'tiles/6175_2_28.png',\n",
       " 'tiles/6175_2_3.png',\n",
       " 'tiles/6175_2_4.png',\n",
       " 'tiles/6175_2_5.png',\n",
       " 'tiles/6175_2_6.png',\n",
       " 'tiles/6175_2_7.png',\n",
       " 'tiles/6175_2_8.png',\n",
       " 'tiles/6175_2_9.png',\n",
       " 'tiles/6175_3_0.png',\n",
       " 'tiles/6175_3_1.png',\n",
       " 'tiles/6175_3_10.png',\n",
       " 'tiles/6175_3_11.png',\n",
       " 'tiles/6175_3_12.png',\n",
       " 'tiles/6175_3_13.png',\n",
       " 'tiles/6175_3_14.png',\n",
       " 'tiles/6175_3_15.png',\n",
       " 'tiles/6175_3_16.png',\n",
       " 'tiles/6175_3_17.png',\n",
       " 'tiles/6175_3_18.png',\n",
       " 'tiles/6175_3_19.png',\n",
       " 'tiles/6175_3_2.png',\n",
       " 'tiles/6175_3_20.png',\n",
       " 'tiles/6175_3_21.png',\n",
       " 'tiles/6175_3_22.png',\n",
       " 'tiles/6175_3_23.png',\n",
       " 'tiles/6175_3_24.png',\n",
       " 'tiles/6175_3_25.png',\n",
       " 'tiles/6175_3_26.png',\n",
       " 'tiles/6175_3_27.png',\n",
       " 'tiles/6175_3_28.png',\n",
       " 'tiles/6175_3_3.png',\n",
       " 'tiles/6175_3_4.png',\n",
       " 'tiles/6175_3_5.png',\n",
       " 'tiles/6175_3_6.png',\n",
       " 'tiles/6175_3_7.png',\n",
       " 'tiles/6175_3_8.png',\n",
       " 'tiles/6175_3_9.png',\n",
       " 'tiles/6175_4_0.png',\n",
       " 'tiles/6175_4_1.png',\n",
       " 'tiles/6175_4_10.png',\n",
       " 'tiles/6175_4_11.png',\n",
       " 'tiles/6175_4_12.png',\n",
       " 'tiles/6175_4_13.png',\n",
       " 'tiles/6175_4_14.png',\n",
       " 'tiles/6175_4_15.png',\n",
       " 'tiles/6175_4_16.png',\n",
       " 'tiles/6175_4_17.png',\n",
       " 'tiles/6175_4_18.png',\n",
       " 'tiles/6175_4_19.png',\n",
       " 'tiles/6175_4_2.png',\n",
       " 'tiles/6175_4_20.png',\n",
       " 'tiles/6175_4_21.png',\n",
       " 'tiles/6175_4_22.png',\n",
       " 'tiles/6175_4_23.png',\n",
       " 'tiles/6175_4_24.png',\n",
       " 'tiles/6175_4_25.png',\n",
       " 'tiles/6175_4_26.png',\n",
       " 'tiles/6175_4_27.png',\n",
       " 'tiles/6175_4_28.png',\n",
       " 'tiles/6175_4_3.png',\n",
       " 'tiles/6175_4_4.png',\n",
       " 'tiles/6175_4_5.png',\n",
       " 'tiles/6175_4_6.png',\n",
       " 'tiles/6175_4_7.png',\n",
       " 'tiles/6175_4_8.png',\n",
       " 'tiles/6175_4_9.png',\n",
       " 'tiles/6175_5_0.png',\n",
       " 'tiles/6175_5_1.png',\n",
       " 'tiles/6175_5_10.png',\n",
       " 'tiles/6175_5_11.png',\n",
       " 'tiles/6175_5_12.png',\n",
       " 'tiles/6175_5_13.png',\n",
       " 'tiles/6175_5_14.png',\n",
       " 'tiles/6175_5_15.png',\n",
       " 'tiles/6175_5_16.png',\n",
       " 'tiles/6175_5_17.png',\n",
       " 'tiles/6175_5_18.png',\n",
       " 'tiles/6175_5_19.png',\n",
       " 'tiles/6175_5_2.png',\n",
       " 'tiles/6175_5_20.png',\n",
       " 'tiles/6175_5_21.png',\n",
       " 'tiles/6175_5_22.png',\n",
       " 'tiles/6175_5_23.png',\n",
       " 'tiles/6175_5_24.png',\n",
       " 'tiles/6175_5_25.png',\n",
       " 'tiles/6175_5_26.png',\n",
       " 'tiles/6175_5_27.png',\n",
       " 'tiles/6175_5_28.png',\n",
       " 'tiles/6175_5_3.png',\n",
       " 'tiles/6175_5_4.png',\n",
       " 'tiles/6175_5_5.png',\n",
       " 'tiles/6175_5_6.png',\n",
       " 'tiles/6175_5_7.png',\n",
       " 'tiles/6175_5_8.png',\n",
       " 'tiles/6175_5_9.png',\n",
       " 'tiles/6175_6_0.png',\n",
       " 'tiles/6175_6_1.png',\n",
       " 'tiles/6175_6_10.png',\n",
       " 'tiles/6175_6_11.png',\n",
       " 'tiles/6175_6_12.png',\n",
       " 'tiles/6175_6_13.png',\n",
       " 'tiles/6175_6_14.png',\n",
       " 'tiles/6175_6_15.png',\n",
       " 'tiles/6175_6_16.png',\n",
       " 'tiles/6175_6_17.png',\n",
       " 'tiles/6175_6_18.png',\n",
       " 'tiles/6175_6_19.png',\n",
       " 'tiles/6175_6_2.png',\n",
       " 'tiles/6175_6_20.png',\n",
       " 'tiles/6175_6_21.png',\n",
       " 'tiles/6175_6_22.png',\n",
       " 'tiles/6175_6_23.png',\n",
       " 'tiles/6175_6_24.png',\n",
       " 'tiles/6175_6_25.png',\n",
       " 'tiles/6175_6_26.png',\n",
       " 'tiles/6175_6_27.png',\n",
       " 'tiles/6175_6_28.png',\n",
       " 'tiles/6175_6_3.png',\n",
       " 'tiles/6175_6_4.png',\n",
       " 'tiles/6175_6_5.png',\n",
       " 'tiles/6175_6_6.png',\n",
       " 'tiles/6175_6_7.png',\n",
       " 'tiles/6175_6_8.png',\n",
       " 'tiles/6175_6_9.png',\n",
       " 'tiles/6175_7_0.png',\n",
       " 'tiles/6175_7_1.png',\n",
       " 'tiles/6175_7_10.png',\n",
       " 'tiles/6175_7_11.png',\n",
       " 'tiles/6175_7_12.png',\n",
       " 'tiles/6175_7_13.png',\n",
       " 'tiles/6175_7_14.png',\n",
       " 'tiles/6175_7_15.png',\n",
       " 'tiles/6175_7_16.png',\n",
       " 'tiles/6175_7_17.png',\n",
       " 'tiles/6175_7_18.png',\n",
       " 'tiles/6175_7_19.png',\n",
       " 'tiles/6175_7_2.png',\n",
       " 'tiles/6175_7_20.png',\n",
       " 'tiles/6175_7_21.png',\n",
       " 'tiles/6175_7_22.png',\n",
       " 'tiles/6175_7_23.png',\n",
       " 'tiles/6175_7_24.png',\n",
       " 'tiles/6175_7_25.png',\n",
       " 'tiles/6175_7_26.png',\n",
       " 'tiles/6175_7_27.png',\n",
       " 'tiles/6175_7_28.png',\n",
       " 'tiles/6175_7_3.png',\n",
       " 'tiles/6175_7_4.png',\n",
       " 'tiles/6175_7_5.png',\n",
       " 'tiles/6175_7_6.png',\n",
       " 'tiles/6175_7_7.png',\n",
       " 'tiles/6175_7_8.png',\n",
       " 'tiles/6175_7_9.png',\n",
       " 'tiles/6175_8_0.png',\n",
       " 'tiles/6175_8_1.png',\n",
       " 'tiles/6175_8_10.png',\n",
       " 'tiles/6175_8_11.png',\n",
       " 'tiles/6175_8_12.png',\n",
       " 'tiles/6175_8_13.png',\n",
       " 'tiles/6175_8_14.png',\n",
       " 'tiles/6175_8_15.png',\n",
       " 'tiles/6175_8_16.png',\n",
       " 'tiles/6175_8_17.png',\n",
       " 'tiles/6175_8_18.png',\n",
       " 'tiles/6175_8_19.png',\n",
       " 'tiles/6175_8_2.png',\n",
       " 'tiles/6175_8_20.png',\n",
       " 'tiles/6175_8_21.png',\n",
       " 'tiles/6175_8_22.png',\n",
       " 'tiles/6175_8_23.png',\n",
       " 'tiles/6175_8_24.png',\n",
       " 'tiles/6175_8_25.png',\n",
       " 'tiles/6175_8_26.png',\n",
       " 'tiles/6175_8_27.png',\n",
       " 'tiles/6175_8_28.png',\n",
       " 'tiles/6175_8_3.png',\n",
       " 'tiles/6175_8_4.png',\n",
       " 'tiles/6175_8_5.png',\n",
       " 'tiles/6175_8_6.png',\n",
       " 'tiles/6175_8_7.png',\n",
       " 'tiles/6175_8_8.png',\n",
       " 'tiles/6175_8_9.png',\n",
       " 'tiles/6175_9_0.png',\n",
       " 'tiles/6175_9_1.png',\n",
       " 'tiles/6175_9_10.png',\n",
       " 'tiles/6175_9_11.png',\n",
       " 'tiles/6175_9_12.png',\n",
       " 'tiles/6175_9_13.png',\n",
       " 'tiles/6175_9_14.png',\n",
       " 'tiles/6175_9_15.png',\n",
       " 'tiles/6175_9_16.png',\n",
       " 'tiles/6175_9_17.png',\n",
       " 'tiles/6175_9_18.png',\n",
       " 'tiles/6175_9_19.png',\n",
       " 'tiles/6175_9_2.png',\n",
       " 'tiles/6175_9_20.png',\n",
       " 'tiles/6175_9_21.png',\n",
       " 'tiles/6175_9_22.png',\n",
       " 'tiles/6175_9_23.png',\n",
       " 'tiles/6175_9_24.png',\n",
       " 'tiles/6175_9_25.png',\n",
       " 'tiles/6175_9_26.png',\n",
       " 'tiles/6175_9_27.png',\n",
       " 'tiles/6175_9_28.png',\n",
       " 'tiles/6175_9_3.png',\n",
       " 'tiles/6175_9_4.png',\n",
       " 'tiles/6175_9_5.png',\n",
       " 'tiles/6175_9_6.png',\n",
       " 'tiles/6175_9_7.png',\n",
       " 'tiles/6175_9_8.png',\n",
       " 'tiles/6175_9_9.png']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_files = sorted(glob(str(\"tiles/*.png\")))\n",
    "tile_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f99dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84081000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    self._close()\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 240, in _feed\n",
      "    self.run()\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/threading.py\", line 975, in run\n",
      "    writer_close()\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/connection.py\", line 360, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 271, in _feed\n",
      "    self.run()\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/threading.py\", line 975, in run\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "    self.run()\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 271, in _feed\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights loaded\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(CFG.img_size, CFG.img_size),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}\n",
    "\n",
    "\n",
    "valid_dataset = UBCSegDataset(tile_files, transforms=data_transforms[\"valid\"])\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=CFG.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "model_name = \"efficientnet-b0\"\n",
    "model_weight = \"exp-seg-2/seg-fold-0.pth\"\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=model_name,\n",
    "    encoder_weights=None,\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None,\n",
    ")\n",
    "\n",
    "    \n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(model_weight))\n",
    "print(\"weights loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d249d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2286052/2060729563.py\", line 1, in <module>\n",
      "    for index, data in enumerate(valid_loader):\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 441, in __iter__\n",
      "    return self._get_iterator()\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 388, in _get_iterator\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1084, in __init__\n",
      "    self._reset(loader, first_iter=True)\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1117, in _reset\n",
      "    self._try_put_index()\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1351, in _try_put_index\n",
      "    index = self._next_index()\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 623, in _next_index\n",
      "    return next(self._sampler_iter)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/torch/utils/data/sampler.py\", line 254, in __iter__\n",
      "    for idx in self.sampler:\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/torch/utils/data/sampler.py\", line 76, in __iter__\n",
      "    return iter(range(len(self.data_source)))\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2286052/4121159660.py\", line 8, in __len__\n",
      "    return len(self.df)\n",
      "               ^^^^^^^\n",
      "AttributeError: 'UBCSegDataset' object has no attribute 'df'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1115, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 424, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/shigengtian/anaconda3/lib/python3.11/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "for index, data in enumerate(valid_loader):\n",
    "    image = data[\"image\"].to(device)\n",
    "    pred = model(image)\n",
    "    pred = torch.sigmoid(pred)\n",
    "    preds = pred.detach().cpu().numpy()\n",
    "            \n",
    "    threshold = 0.33\n",
    "    binary_masks = (preds > threshold).astype(int)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e7011dc-c63d-4360-af4f-c1441a3c6c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'\n",
    "    \n",
    "    \n",
    "class UBCModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=5, pretrained=False, checkpoint_path=None):\n",
    "        super(UBCModel, self).__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "        in_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "        self.model.global_pool = nn.Identity()\n",
    "        self.pooling = GeM()\n",
    "        self.linear = nn.Linear(in_features, num_classes)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.model(images)\n",
    "        pooled_features = self.pooling(features).flatten(1)\n",
    "        output = self.linear(pooled_features)\n",
    "        return output\n",
    "    \n",
    "class UBCClcDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.file_names = df[\"tile_file_paths\"].values\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_names[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "\n",
    "        return {\"image\": img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f96aa0f-978e-4775-b7e2-6ac26f5ade5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def infer_clc(tile_df, model):\n",
    "\n",
    "\n",
    "    def get_transforms():\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.img_size, CFG.img_size),\n",
    "                A.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225],\n",
    "                    max_pixel_value=255.0,\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                ToTensorV2(),\n",
    "            ],\n",
    "            p=1.0,\n",
    "        )\n",
    "\n",
    "    test_dataset = UBCClcDataset(tile_df, transforms=get_transforms())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, \n",
    "                              num_workers=4, shuffle=False, pin_memory=True)\n",
    "    probs = []\n",
    "    for index, data in enumerate(test_loader):\n",
    "        data = data[\"image\"].to(device)\n",
    "        outputs = model(data)\n",
    "        p = torch.sigmoid(outputs)\n",
    "        p = p.detach().cpu().numpy()\n",
    "        probs.append(p)\n",
    "        \n",
    "        del data\n",
    "        del outputs\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    probs = np.concatenate(probs)\n",
    "    probs = np.mean(probs,0)\n",
    "    max_index = np.argmax(probs)\n",
    "    \n",
    "    label_dict = {\n",
    "        0 : \"CC\",\n",
    "        1 : \"EC\",\n",
    "        2 : \"HGSC\",\n",
    "        3 : \"LGSC\",\n",
    "        4 : \"MC\"\n",
    "    }\n",
    "    \n",
    "    max_prob = probs[max_index]\n",
    "\n",
    "    if max_prob > 0.25:\n",
    "        pred_class = label_dict[max_index]\n",
    "    else:\n",
    "        pred_class = \"Other\"\n",
    "    return pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885f418-6496-43c3-a415-c1a04ad26b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shigengtian/anaconda3/lib/python3.11/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name tf_efficientnetv2_s_in21ft1k to current tf_efficientnetv2_s.in21k_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights loaded\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "tile_size = 2048\n",
    "\n",
    "clc_weight = \"exp-06/fold-0.pth\"\n",
    "model = UBCModel(\"tf_efficientnetv2_s_in21ft1k\")\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(clc_weight))\n",
    "print(\"weights loaded\")\n",
    "rst_image_ids = []\n",
    "rst_labels=[]\n",
    "for index, row in test_df.iterrows():\n",
    "    save_path = \"tiles\"\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    _img_ids = []\n",
    "    _tile_file_paths = []\n",
    "    \n",
    "    image_id = row[\"image_id\"]\n",
    "    img_path = row[\"file_path\"]\n",
    "    mask_path = row[\"seg_paths\"]\n",
    "    is_tma = row[\"is_tma\"]\n",
    "    \n",
    "    \n",
    "    img = cv2.imread(str(img_path))\n",
    "    \n",
    "    height, width = img.shape[:2]\n",
    "    mask = np.load(str(mask_path))\n",
    "    mask = cv2.resize(mask, (width, height),interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    rows = height // tile_size\n",
    "    cols = width // tile_size\n",
    "    print(is_tma)\n",
    "#     if is_tma:\n",
    "#         tile = cv2.imread(img_path)\n",
    "#         tile = cv2.resize(tile, (512, 512), interpolation=cv2.INTER_NEAREST)\n",
    "#         tile_filename = f\"{image_id}.png\"\n",
    "#         tile_path = f\"{save_path}/{tile_filename}\"\n",
    "#         cv2.imwrite(tile_path, tile)\n",
    "#         _img_ids.append(image_id)\n",
    "#         _tile_file_paths.append(tile_path)\n",
    "#         del tile\n",
    "#         gc.collect()\n",
    "#         pass\n",
    "#     else:\n",
    "#         for i in range(rows):\n",
    "#             for j in range(cols):\n",
    "#                 tile_mask = mask[\n",
    "#                     i * tile_size : (i + 1) * tile_size,\n",
    "#                     j * tile_size : (j + 1) * tile_size,\n",
    "#                 ]\n",
    "#                 true_percentage = tile_mask.sum() / tile_mask.size\n",
    "#                 if true_percentage < 0.5:\n",
    "#                     continue\n",
    "\n",
    "#                 tile = img[\n",
    "#                     i * tile_size : (i + 1) * tile_size,\n",
    "#                     j * tile_size : (j + 1) * tile_size,\n",
    "#                 ]\n",
    "#                 tile = cv2.resize(tile, (512, 512), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#                 tile_filename = f\"{image_id}_{i}_{j}.png\"\n",
    "#                 tile_path = f\"{save_path}/{tile_filename}\"\n",
    "#                 cv2.imwrite(tile_path, tile)\n",
    "#                 _img_ids.append(image_id)\n",
    "#                 _tile_file_paths.append(tile_path)\n",
    "#                 del tile\n",
    "#                 gc.collect()\n",
    "                \n",
    "                \n",
    "\n",
    "#     del img\n",
    "#     del mask\n",
    "#     gc.collect()\n",
    "    \n",
    "#     tile_df = pd.DataFrame()\n",
    "#     tile_df[\"image_id\"] = _img_ids\n",
    "#     tile_df[\"tile_file_paths\"] = _tile_file_paths\n",
    "#     print(tile_df)\n",
    "#     pred_class = infer_clc(tile_df, model)\n",
    "#     rst_image_ids.append(int(image_id))\n",
    "#     rst_labels.append(pred_class)\n",
    "#     shutil.rmtree(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7369ce-a033-4f7c-a58e-867148847e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
